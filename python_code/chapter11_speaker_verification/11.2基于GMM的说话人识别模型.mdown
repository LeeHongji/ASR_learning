高斯混合模型(Gaussian Mixture Model, GMM)可以看做一种状态数为1的连续分布隐马尔科夫模型。一个$M$阶混合高斯模型的概率密度函数是$M$个高斯概率密度函数加权求和得到的:
$$P(X|\lambda)=\sum_{i=1}^Mw_ib_i(X)$$

其中$X$是D维随机向量，为说话人识别算法提取的特征矢量，$w_i$是混合权重，满足$\sum_{i=1}^Mw_i=1$,每个分布因子$b_i(X_t)$是D维的联合高斯概率分布：
$$b_i(X)=\frac{1}{(2\pi)^{D/2}|\sum_i|^{1/2}}\exp\{-\frac{1}{2}(X-\mu_i)^t(\sum)_i^{-1}(X-\mu_i)\}$$

$\mu_i$是均值向量，$(\sum)_i$是协方差矩阵。$\lambda=\{w_i,\mu_i,\sum_i\},i=1,2,...,M$

GMM常用的参数确定方法是极大似然估计，在具体实施时候，通常采用期望值最大算法估计参数$\lambda$,采用EM算法估计一个新的参数$\hat \lambda$,使得新参数模型下的似然度$P(X|\hat \lambda)\geqslant P(X|\lambda)$，迭代到模型收敛。每次迭代三组参数的重估公式为：
混合全職重估：
$$w_i=\frac{1}{T}\sum_{t=1}^TP(i|X_t,\lambda)$$

均值重估：
$$\mu_i=\frac{\sum_{t=1}^TP(i|X_t,\lambda)X_t}{\sum_{t=1}^TP(i|X_t,\lambda)}$$

方差重估：
$$\sigma_i^2=\frac{\sum_{t=1}^TP(i|X_t,\lambda)(X_t-\mu_i)^2}{\sum_{t=1}^TP(i|X_t,\lambda)}$$

其中，分量的後驗概率為：
$$P(i|X_t,\lambda)=\frac{w_ib_i(X_t)}{\sum_{k=1}^Mw_kb_k(X_t)}$$

基于GMM的说话人识别模型为：
 - 对每个说话人$n=1,2,...,N$的训练语音集，提取特征参数，基于最大似然准则，通过EM算法，建立与改说话人对应的高斯混合模型参数$\lambda_1,\lambda_2,...,\lambda_N$
 - 对于带识别说话人的语音，提取特征参数，然后计算其关于训练后得到的每一个说话人的模型$\lambda_1,\lambda_2,...,\lambda_N$的似然值$p(X|\lambda_n)$,将其中最大似然值对应的需要作为说话人的识别结果$n^*=\arg \underset{n}{\max}p(X|\lambda_n)$

还要注意：
GMM模型的高斯分布个数M和模型初始参数必须首先确定，M一般又试验得来，可取$M=4,8,16$。参数初始化一般采用聚类的方式将特征矢量归为与混合数相等的各类中，然后分别计算各个类的方差和均值，作为初始矩阵和均值,方差矩阵可以是全矩阵，也可以是对角矩阵。
在实际应用中，往往得不到大量充分的训练数据对模型参数进行训练，由于训练数据的不充分，GMM模型的协方差矩阵的一些分量可能会很小，他们会严重影响系统性能。为了避免小的值对系统性能的影响，在EM算法的迭代计算中，对协方差的值设定一个门限值，在先练过程中令协方差的值不小于门限值。



