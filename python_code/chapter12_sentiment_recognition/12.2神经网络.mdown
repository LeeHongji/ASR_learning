人工神经网络(Artificial Neural Network , ANN) 是一种由大量简单处理单元构成的并行分布式数学模型。在人类意识到人脑计算与传统计算机处理方式的区别时，神经网络就成了科学家们探究信息处理任务的关注对象。人工神经网络主要从两方面模仿大脑工作:从外界环境中学习，用突触权值存储知识。人类大脑接收外界剌激，感受器转换为电冲击传递给神经元构成的网络，再经由效应器把电冲击转换为可识别的效应输出。

神经元是神经网络处理信息的基本单位，是由突触权值、加法器、激活函数三部分构成的非线性模型。

神经网络的训练就是反向传播的过程，最终的误差为$\epsilon(n)=\frac{1}{2}\sum_{j\in C}e_j^2(n)$,那么更新权重$w_{ij}(n)$的方式就是求偏导$\Delta w_{ij}=\frac{\partial \epsilon}{\partial w_{ij}}$，使用梯度下降：
$$w_{ij}=w_{ij}-\alpha\Delta w_{ij}$$

### 概率神经网络
概率神经网络(Probabilistic Neural Network, PNN)的主要思想是利用贝叶斯决策规则，即错误分类期望最小，在多维输入空间内分离决策空间。是一种基于统计原理的人工神经网络，使用Parzen窗口函数作为激活函数。

贝叶斯决策理论：
$$if p(w_i|\overrightarrow{x})>p(w_j|\overrightarrow{x}),\forall j\neq i,\overrightarrow{x}\in w_i,then \overrightarrow{x}\in w_i$$

其中：$p(w_i|\overrightarrow{x})=p(w_i)p(\overrightarrow{x}|w_j)$

由于类的概率密度函数$p(w_j|\overrightarrow{x})$是未知的，用高斯核的Parzen估计：
$$p(\overrightarrow{x}|w_i)=\frac{1}{N_i}\sum_{k=1}^{N_i}\frac{1}{(2\pi)^{l/2}\sigma^l}\exp{(\frac{||\overrightarrow{x}-\overrightarrow{x}_{ik}||^2}{2\sigma^2})}$$

其中，$\overrightarrow{x}_{ik}$是属于$w_i$类的第k个训练样本，l是样本向量的维度，$\sigma$是平滑参数，$N_i$是第$2_i$类的训练样本总量。去掉常数项，判别函数可以简化为：
$$g_i(\overrightarrow{x})=\frac{p(w_i)}{N_i}\sum_{k=1}^{N_i}\exp{(-\frac{||\overrightarrow{x}-\overrightarrow{x}_{ik}||^2}{2\sigma^2})}$$

PNN网络有四个部分，输入层，样本层，求和层，竞争层。首先将输入向量$\overrightarrow{x}$输入到输入层，在输入层中，网络计算输入向量与训练样本向量之间的差值$\overrightarrow{x}-\overrightarrow{x}_{ik}$,差值绝对值$||\overrightarrow{x}-\overrightarrow{x}_{ik}||$的大小代表着两个向量之间的距离，输入层的输出向量$\overrightarrow{x}-\overrightarrow{x}_{ik}$送入样本层，样本层节点数量等于训练样本数量的总和。样本层的主要工作是：判断那些类别与输入向量有关，将相关度高的类别集中起来，样本册的输出值表示了相识度，送入求和层，求和层的节点个数为类的数量，每个节点对应一个类，通过求和层的竞争传递函数进行判决，最后的判决结果由竞争层输出。每个结果只有一个1，其余都是0，概率值最大的那一类的结果为1.
### LVQ神经网络
学习向量量化( Learning Vector Quantization , LVQ)神经网络，属于前向有监督神经网络类型，在模式识别和优化领域有着广泛的应用。LVQ神经网络由输入层、隐含层和输出层气层组成，输入层与隐含层间为完全连接，每个输出层神经元与隐含层神经元的不同组相连接。隐含层和输出层神经元之间的连接权值同定为l。在网络训练过程中，输入层和隐含层神经元间的权值被修改。当某个输入模式被送至网络时，最接近输入模式的隐含神经元因获得激发而赢得竞争，因而允许它产生一个"1"，而其他隐含层神经元都被迫产生"0"。与包含获胜神经元的隐含层神经元组相连接的输出神经元也发出"1" ，而其他输出神经元均发出"0"。

#### LVQ1
1)网络初始化。用较小的随机数设定输入层和隐含层之间的权值初始值。
2)输入向量的输入。将输入向量$X=[x_1,x_2,...,x_n]^T$送入到输入层。
3)计算隐含层权值向量与输入向量的距离$d_j=\sqrt{\sum_{i=1}^n(x_i-w_{ij})^2}$
4)选择与权值向量的距离最小的神经元。计-算并选择输入向量和权值向量的距离最小的
神经元，并把其称为胜出神经元，记为$j^*$。
5)更新连接权值。如果胜出神经元和预先指定的分类一致，称为正确分类，否则称为
不正确分类。正确分类和不正确分类时权值的调整量分别使用如下公式:
$$\Delta w_{ij}=\left \{\begin{array}{ll}
    +\alpha(x_i-w{ij})& 分类正确时\\-\alpha(x_i-w{ij})& 分类不正确时
\end{array}\right.$$

6)达到最大迭代次数结束。

#### LVQ2
LVQ2的算法其他内容与LVQ1一样，就第5点不一样：
5)更新链接权值，如果胜出神经元1属于正确分类，则权值更新与LVQ1相同，如果不是正确分类，则选取另一个神经元2，他的权值向量与输入向量的距离只比神经元1的大一点，那么在满足限量两个条件时使用更新表达式更新。
 - 神经元2输入正确分类
 - 神经元2，胜出神经元1与输入向量的距离差很小。